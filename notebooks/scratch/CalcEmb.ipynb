{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b90e857-e916-42f9-b9f5-b21329f7a7d8",
   "metadata": {},
   "source": [
    "```bash\n",
    "uv pip install torch torchvision tensorboardx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "292881ab-b0e1-4e15-8d70-472f2524e46d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T02:41:39.222152Z",
     "start_time": "2025-04-21T02:41:39.220010Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from gait import Column, Layer, Layers, FEL\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59fbb07b-4a61-4979-a567-fcde713251fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T02:40:44.425501Z",
     "start_time": "2025-04-21T02:40:44.423196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available!\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS is available!\")\n",
    "else:\n",
    "    print(\"MPS is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ac1ea26-fda3-4f62-a093-390199a1cbe2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T02:42:23.547839Z",
     "start_time": "2025-04-21T02:42:23.544930Z"
    }
   },
   "outputs": [],
   "source": [
    "layers = Layers.load(os.path.expanduser(\"~/data/NorthSea.json\"))\n",
    "fel = FEL(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b5158e-79f6-44e8-90d5-99e20a4bbdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "line1 = {layers.create_line_1() for _ in range(2000)}\n",
    "line2 = {layers.create_line_2() for _ in range(2000)}\n",
    "\n",
    "data.extend([(_, 0) for _ in line1])\n",
    "data.extend([(_, 1) for _ in line2])\n",
    "\n",
    "random.shuffle(data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa83ca-7d30-4570-9861-c35ec3d59913",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticRoutingEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_routes):\n",
    "        super(SemanticRoutingEmbeddingModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # Learnable embeddings\n",
    "        self.fc = nn.Linear(embed_dim, num_routes)  # Map embeddings to route space\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Convert tokens to embeddings\n",
    "        x = x.mean(dim=1)  # Pooling (mean or max pooling over sequence)\n",
    "        x = F.normalize(x, p=2, dim=1)  # Normalize embeddings\n",
    "        logits = self.fc(x)  # Route ID logits\n",
    "        return x, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90add43-6ebf-4a41-b266-24571ff2bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # Calculate Euclidean distance\n",
    "        distance = F.pairwise_distance(output1, output2)\n",
    "        # Contrastive loss\n",
    "        loss = (label * distance**2) + (\n",
    "            (1 - label) * F.relu(self.margin - distance) ** 2\n",
    "        )\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e748198e-fcad-4dc9-b1d4-7d465b57e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss().to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3f4690-dd5e-4e11-aeaa-d77dc62aba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Prepare data\n",
    "            inputs, route_ids = batch  # For classification\n",
    "            # inputs, inputs2, labels = batch  # For contrastive loss\n",
    "\n",
    "            # Forward pass\n",
    "            embeddings, logits = model(inputs)\n",
    "            loss = criterion(logits, route_ids)  # Use appropriate loss function\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e66bf72-8c4c-4a8c-bd30-f6d4c97f3d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticRouteDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=360):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, meta = self.data[idx]\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "        return tokens, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada4dd57-ecc4-4096-81ee-725610f480ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92868ca0-f93e-4b38-81f3-d938c309c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = SemanticRouteDataset(data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecb4029-26af-4ba4-b594-a44c2530f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144f42b-08cb-4665-a275-b408b6a05e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SemanticRoutingEmbeddingModel(tokenizer.vocab_size, 768, 2).to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d792d5c-88c9-4b04-8082-d614dcb8c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        _, logits = model(inputs)  # Forward pass\n",
    "\n",
    "        loss = criterion(logits, targets)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
